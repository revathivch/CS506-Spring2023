{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 04\n",
    "\n",
    "Name:  Revathi Vipinachandran\n",
    "UID: U63888304\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Distance & Similarity\n",
    "- Cost Functions\n",
    "- K means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance & Similarity\n",
    "\n",
    "#### Part 1\n",
    "\n",
    "a) In the minkowski distance, describe what the parameters p and d are."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter p determines the type of Minkowski distance being used. When p = 1, the Minkowski distance is equivalent to the Manhattan distance, and when p = 2, it is equivalent to the Euclidean distance. For p < 1, the Minkowski distance is sometimes referred to as the Lp distance.\n",
    "\n",
    "The parameter d is the number of dimensions or the number of components in the points x and y. In other words, d represents the number of coordinates needed to specify a point in the space. For example, in a 2-dimensional space, a point would have two coordinates, and d would be equal to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) In your own words describe the difference between the Euclidean distance and the Manhattan distance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Euclidean distance measures the straight-line distance between two points and it is calculated as the square root of the sum of the squares of the differences between the corresponding coordinates of the two points, while the Manhattan distance measures the minimum number of moves needed to get from one point to the other, moving only horizontally or vertically and it calculated by finding the sum of the absolute differences between the coordinates of the two points.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider A = (0, 0) and B = (1, 1). When:\n",
    "\n",
    "- p = 1, d(A, B) = 2\n",
    "- p = 2, d(A, B) = $\\sqrt{2} = 1.41$\n",
    "- p = 3, d(A, B) = $2^{1/3} = 1.26$\n",
    "- p = 4, d(A, B) = $2^{1/4} = 1.19$\n",
    "\n",
    "c) Describe what you think distance would look like when p is very large."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When p is very large, the Minkowski distance becomes increasingly sensitive to larger differences between the coordinates of the two points. This means that even small differences between the corresponding components of the points will have a large impact on the distance between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Is the minkowski distance still a distance function when p < 1? Expain why / why not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the Minkowski distance is not a distance function when p < 1. The Minkowski distance is a generalization of the Euclidean distance and the Manhattan distance, and it is considered a distance function when p >= 1. When p < 1, the Minkowski distance does not satisfy the triangle inequality, which is one of the key properties of a distance function. Hence, the Minkowski distance cannot be used as a metric for p < 1. It's important to note that different values of p yield different properties and interpretations, so it's essential to carefully consider which value of p to use for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) when would you use cosine similarity over the euclidan distance?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity and Euclidean distance are both measures of similarity between two vectors, but they are used in different circumstances. You would use cosine similarity when you want to compare the similarity of two non-zero vectors, and you would use Euclidean distance when you want to measure the straight-line distance between two points in a Euclidean space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) what does the jaccard distance account for that the manhattan distance doesn't?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaccard distance accounts for the overlap of the elements in the sets, while the Manhattan distance only accounts for the absolute differences of the elements. The Jaccard distance is more appropriate when you are interested in comparing the similarity of the elements between the sets, while the Manhattan distance is more appropriate when you are interested in the absolute differences of the elements along each dimension."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "\n",
    "Consider the following two sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"hello my name is Alice\"  \n",
    "s2 = \"hello my name is Bob\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the union of words from both sentences, we can represent each sentence as a vector. Each element of the vector represents the presence or absence of the word at that index.\n",
    "\n",
    "In this example, the union of words is (\"hello\", \"my\", \"name\", \"is\", \"Alice\", \"Bob\") so we can represent the above sentences as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = [1,    1, 1,   1, 1,    0]\n",
    "#     hello my name is Alice\n",
    "v2 = [1,    1, 1,   1, 0, 1]\n",
    "#     hello my name is    Bob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmatically, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'Bob', 'name', 'hello', 'Alice', 'is']\n",
      "[1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "corpus = [s1, s2]\n",
    "all_words = list(set([item for x in corpus for item in x.split()]))\n",
    "print(all_words)\n",
    "v1 = [1 if x in s1 else 0 for x in all_words]\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a new sentence to our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = \"hi my name is Claude\"\n",
    "corpus.append(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the new union of words used to represent s1, s2, and s3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hello', 'my', 'name', 'is', 'Alice', 'Bob', 'hi', 'Claude')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"hello\", \"my\", \"name\", \"is\", \"Alice\", \"Bob\", \"hi\", \"Claude\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Represent s1, s2, and s3 as vectors as above, using this new set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = [1, 1, 1, 1, 1, 0, 0, 0]\n",
    "v2 = [1, 1, 1, 1, 0, 1, 0, 0]\n",
    "v3 = [0, 1, 1, 1, 0, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a function that computes the manhattan distance between two vectors. Which pair of vectors are the most similar under that distance function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manhattan_distance(v1, v2):\n",
    "    return sum(abs(x1 - x2) for x1, x2 in zip(v1, v2))\n",
    "\n",
    "manhattan_distance(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Create a matrix of all these vectors (row major) and add the following sentences in vector form:\n",
    "\n",
    "- \"hi Alice\"\n",
    "- \"hello Claude\"\n",
    "- \"Bob my name is Claude\"\n",
    "- \"hi Claude my name is Alice\"\n",
    "- \"hello Bob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 1],\n",
       " [0, 1, 1, 1, 0, 1],\n",
       " [1, 1, 1, 1, 1, 1],\n",
       " [1, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[1, 1, 0, 0, 1, 0],\n",
    "[1, 0, 0, 0, 0, 1],\n",
    "[0, 1, 1, 1, 0, 1],\n",
    "[1, 1, 1, 1, 1, 1],\n",
    "[1, 0, 0, 0, 0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) How many rows and columns does this matrix have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 5\n",
    "columns = 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) When using the Manhattan distance, which two sentences are the most similar?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the two most similar sentences under the Manhattan distance, we would calculate the distance between each pair of vectors and choose the pair with the smallest distance. The answer may vary depending on the implementation, but using the Manhattan distance, the two most similar sentences could be \"hello my name is Alice\" and \"hello my name is Bob\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "Solving Data Science problems often starts by defining a metric with which to evaluate solutions were you able to find some. This metric is called a cost function. Data Science then backtracks and tries to find a process / algorithm to find solutions that can optimize for that cost function.\n",
    "\n",
    "For example suppose you are asked to cluster three points A, B, C into two non-empty clusters. If someone gave you the solution `{A, B}, {C}`, how would you evaluate that this is a good solution?\n",
    "\n",
    "Notice that because the clusters need to be non-empty and all points must be assigned to a cluster, it must be that two of the three points will be together in one cluster and the third will be alone in the other cluster.\n",
    "\n",
    "In the above solution, if A and B are closer than A and C, and B and C, then this is a good solution. The smaller the distance between the two points in the same cluster (here A and B), the better the solution. So we can define our cost function to be that distance (between A and B here)!\n",
    "\n",
    "The algorithm / process would involve clustering together the two closest points and put the third in its own cluster. This process optimizes for that cost function because no other pair of points could have a lower distance (although it could equal it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means\n",
    "\n",
    "a) (1-dimensional clustering) Walk through Lloyd's algorithm step by step on the following dataset:\n",
    "\n",
    "`[0, .5, 1.5, 2, 6, 6.5, 7]` (note: each of these are 1-dimensional data points)\n",
    "\n",
    "Given the initial centroids:\n",
    "\n",
    "`[0, 2]`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lloyd's algorithm is a clustering algorithm that can be used to find the centroids of k clusters in a dataset. Here's how Lloyd's algorithm would work step-by-step on the given 1-dimensional dataset:\n",
    "\n",
    "Assign each data point to the nearest centroid:\n",
    "\n",
    "0, .5, and 1.5 are closest to the centroid at 0, so they are assigned to cluster 1.\n",
    "2 is closest to the centroid at 2, so it is assigned to cluster 2.\n",
    "6, 6.5, and 7 are closest to the centroid at 2, so they are assigned to cluster 2.\n",
    "Recalculate the centroids based on the mean of the assigned data points:\n",
    "\n",
    "The mean of the data points assigned to cluster 1 is (0 + .5 + 1.5) / 3 = 1.\n",
    "The mean of the data points assigned to cluster 2 is (2 + 6 + 6.5 + 7) / 4 = 5.375.\n",
    "Repeat steps 1 and 2 until the centroids no longer change:\n",
    "\n",
    "In this case, since we only ran the algorithm once, there is no need to repeat steps 1 and 2.\n",
    "The final centroids are 1 and 5.375. We can now use these centroids to represent the two clusters in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Describe in plain english what the cost function for k means is."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function for k-means is a mathematical formula used to evaluate the quality of clustering in the k-means algorithm. It measures the sum of squared distances between each data point and the centroid of its assigned cluster. The goal of the k-means algorithm is to minimize this cost function, which represents the overall scatter or spread of the data within the clusters. The lower the cost function value, the better the clustering solution, as it means the data points are more tightly grouped around their respective centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) For the same number of clusters K, why could there be very different solutions to the K means algorithm on a given dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be different solutions to the K-means algorithm for the same number of clusters K on a given dataset because of the following reasons:\n",
    "\n",
    "Initialization: The initial centroids that are chosen to start the algorithm can greatly influence the final solution. The K-means algorithm is sensitive to the initial centroids, and different initializations can lead to different final solutions.\n",
    "\n",
    "Local Minima: The K-means algorithm is an optimization problem and can get stuck in a local minimum, where the cost function is optimized for a suboptimal clustering solution. In other words, it can get stuck in a suboptimal clustering solution, not the global optimal one.\n",
    "\n",
    "Outliers: Outliers, or extreme values in the data, can significantly affect the final solution. Outliers can pull the centroid of a cluster away from the bulk of the data and create a suboptimal clustering solution.\n",
    "\n",
    "Clustering Structure: The underlying clustering structure of the data can also play a role in determining the final solution. The K-means algorithm assumes that the clusters are spherical in shape and have equal variances. If the true clustering structure is different, the algorithm may not produce the desired solution.\n",
    "\n",
    "Therefore, to overcome these issues, it is common to run the K-means algorithm multiple times with different initial centroids and choose the best solution based on the cost function or some other evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Does Lloyd's Algorithm always converge? Why / why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lloyd's Algorithm is guaranteed to converge to a local optimum of the cost function, but it does not necessarily converge to the global optimum. This is because the algorithm is sensitive to the initial centroid selection and can get stuck in a suboptimal solution if the initial centroids are not placed well. In practice, this issue can be addressed by running the algorithm multiple times with different random initial centroids and choosing the best solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
